# surgery-tracker
Code for hand-tracking in surgery video datasets.

Things to do:  
* Select subset of surgery videos from the following [YouTube playlist](https://www.youtube.com/playlist?list=PLegrqXHtHobDKdZDCcao5N9fweWrNIOej)  
  * Do so based on quality, viewpoints, (and time? Maybe keep videos trimmed in a first pass)  
* Run out-of-the-box pre-trained hand trackers on videos. Pretrained trackers include:
  * [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose)  
  * [DensePose](https://github.com/facebookresearch/DensePose)  
  * [GANerated hands](https://handtracker.mpi-inf.mpg.de/projects/GANeratedHands/)    
* Think about how well they do and visualize.
